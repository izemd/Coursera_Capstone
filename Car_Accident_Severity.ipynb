{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Car Severity Accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Introduction: Business Problem](#introduction)\n",
    "* [Data](#data)\n",
    "* [Data Cleaning](#datacleaning)\n",
    "* [Methodology](#methodology)\n",
    "* [Results and Discussion](#results)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction/Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to predict the accident “severity”. Which factors have more impact on the accidents such as weather, road condition, light condition, speding or any other type of accidents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The society as a whole — the accident victims and their families, their employers, insurance firms, emergency and health care personal and many others — is affected by motor vehicle crashes in many ways. It would be great if real-time conditions can be provided to estimate the trip safeness. In this way, it can be decided beforehand if the driver will take the risk, based on reliable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data was collected by Seattle SPOT Traffic Management Division and provided by Coursera via a link. This dataset is updated weekly and is from 2004 to present. It contains information such as severity code, address type, location, collision type, weather, road condition, speeding, among others.\n",
    "\n",
    "There are 194,673 observations and 38 variables in this data set. Since we would like to identify the factors that cause the accident and the level of severity, we will use SEVERITYCODE as our dependent variable Y, and try different combinations of independent variables X to get the result. Since the observations are quite large, we may need to filter out the missing value and delete the unrelated columns first. Then we can select the factor which may have more impact on the accidents, such as address type, weather, road condition, and light condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data-Collisions.csv\", low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing relevant variables\n",
    "\n",
    "From the complete dataset, we will choose only the relevant variables which might have an impact in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data = df[['SEVERITYCODE', 'X', 'Y', 'ADDRTYPE', 'COLLISIONTYPE',\n",
    "               'PERSONCOUNT', 'VEHCOUNT', 'JUNCTIONTYPE',  'WEATHER', 'ROADCOND', 'LIGHTCOND', \n",
    "               'SPEEDING', 'UNDERINFL', 'INATTENTIONIND']]\n",
    "col_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the data and get the frequency of each category for a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_data.columns:\n",
    "    if ((col_data[col].value_counts()/len(col_data[col])) > 0.8).any() == True:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_count(columns, df):\n",
    "    for col in columns:\n",
    "        print(col)\n",
    "        print(df[col].value_counts())\n",
    "        print()\n",
    "\n",
    "data_columns = ['SEVERITYCODE','ADDRTYPE', 'COLLISIONTYPE', 'JUNCTIONTYPE', 'WEATHER', \n",
    " 'ROADCOND','LIGHTCOND', 'SPEEDING', 'UNDERINFL', 'INATTENTIONIND']\n",
    "\n",
    "#Use value_counts() method in each column\n",
    "list_count(data_columns, col_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Some of the categories include 'Other' and 'Unknown' which does not provide enough information. Threfore, we should drop this entries from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterCond = (col_data.LIGHTCOND == 'Other') | (col_data.LIGHTCOND == 'Unknown') | \\\n",
    "                      (col_data.LIGHTCOND == 'Dark - Unknown Lighting') |\\\n",
    "                      (col_data.ROADCOND == 'Other') | (col_data.ROADCOND == 'Unknown') | \\\n",
    "                      (col_data.WEATHER == 'Other') | (col_data.WEATHER == 'Unknown') | \\\n",
    "                      (col_data.JUNCTIONTYPE == 'Other') | (col_data.JUNCTIONTYPE == 'Unknown') | \\\n",
    "                      (col_data.COLLISIONTYPE == 'Other')\n",
    "col_data = col_data.drop(col_data[filterCond].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data[\"LIGHTCOND\"] = col_data[\"LIGHTCOND\"].replace(\"Dark - Street Lights Off\", \"Dark - No Street Lights\")\n",
    "col_data[\"UNDERINFL\"] = col_data[\"UNDERINFL\"].replace(\"N\", 0)\n",
    "col_data[\"UNDERINFL\"] = col_data[\"UNDERINFL\"].replace(\"0\", 0)\n",
    "col_data[\"UNDERINFL\"] = col_data[\"UNDERINFL\"].replace(\"1\", 1)\n",
    "col_data[\"UNDERINFL\"] = col_data[\"UNDERINFL\"].replace(\"Y\", 1)\n",
    "col_data[\"INATTENTIONIND\"] = col_data[\"INATTENTIONIND\"].replace(\"Y\", 1)\n",
    "col_data[\"SPEEDING\"] = col_data[\"SPEEDING\"].replace(\"Y\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns which has NaN values\n",
    "col_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many missing values, especially in 'Speeding' and 'Inattentionid' column. We will fill them as 0. (Boolean columns with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data['UNDERINFL'] = col_data['UNDERINFL'].fillna(0)\n",
    "col_data['INATTENTIONIND'] = col_data['INATTENTIONIND'].fillna(0)\n",
    "col_data['SPEEDING'] = col_data['SPEEDING'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is cleaned. We have 143.741 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data['SEVERITYCODE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename severitycode to 0,1\n",
    "col_data[\"SEVERITYCODE\"] = col_data[\"SEVERITYCODE\"].replace(1, 0)\n",
    "col_data[\"SEVERITYCODE\"] = col_data[\"SEVERITYCODE\"].replace(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for the relevant dataset\n",
    "feature = pd.concat([pd.get_dummies(col_data['WEATHER']), \n",
    "                     pd.get_dummies(col_data['ROADCOND']),\n",
    "                     pd.get_dummies(col_data['LIGHTCOND'])], axis=1)\n",
    "feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=\"ADDRTYPE\", hue=\"SEVERITYCODE\", data=col_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot represents that 'block' areas have more property damage than intersection areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax= sns.countplot(x=\"COLLISIONTYPE\", hue=\"SEVERITYCODE\", data=col_data)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most accidents happened in parked cars with property damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(y=\"JUNCTIONTYPE\", hue=\"SEVERITYCODE\", data=col_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax= sns.countplot(x=\"WEATHER\", hue=\"SEVERITYCODE\", data=col_data)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(y=\"ROADCOND\", hue=\"SEVERITYCODE\", data=col_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, the most property damage happened in clear weather. Snowing, rainy and other weather conditions have very low. Moreover, dry road condition has more property damage than other road conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(y=\"LIGHTCOND\", hue=\"SEVERITYCODE\", data=col_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In daylight condition has more accidents than other light conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "sns.countplot(x=\"SPEEDING\", hue=\"SEVERITYCODE\", data=col_data, ax=axes[0])\n",
    "sns.countplot(x=\"UNDERINFL\", hue=\"SEVERITYCODE\", data=col_data, ax=axes[1])\n",
    "sns.countplot(x=\"INATTENTIONIND\", hue=\"SEVERITYCODE\", data=col_data, ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge folium=0.5.0 --yes\n",
    "import folium\n",
    "\n",
    "print('Folium installed and imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "seattle_long= -122.335167\n",
    "seattle_lat= 47.608013\n",
    "seattle_map = folium.Map(location=[seattle_lat, seattle_long], zoom_start=4)\n",
    "# let's start again with a clean copy of the map of Seattle\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents = plugins.MarkerCluster().add_to(seattle_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, in zip(col_data.Y, col_data.X):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        #popup=label,\n",
    "    ).add_to(incidents)\n",
    "\n",
    "# display map\n",
    "seattle_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will explore which areas and conditions in Seattle that cause accidents.\n",
    "\n",
    "In first step we have analyzed the data using charts and tables.\n",
    "\n",
    "Second step in our analysis will be exploration of 'severity density' across different type of conditions in Seattle - we will use countplot to identify which factors have more impact on the property damage.\n",
    "\n",
    "In third and final step we will focus on most property damage conditions and within those create clusters of those conditions and find most accurate model to predict it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X matrix and y vector\n",
    "X = feature\n",
    "y = col_data['SEVERITYCODE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing and splitting data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "ConfustionMx = [];\n",
    "for n in range(1,Ks):\n",
    "    \n",
    "    #Train Model and Predict  \n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "    yhat=neigh.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.legend(('Accuracy ', '+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Nabors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "neigh6 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n",
    "yhat6 = neigh6.predict(X_test)\n",
    "print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh6.predict(X_train)))\n",
    "print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted y\n",
    "yhat_knn = neigh.predict(X_test)\n",
    "\n",
    "# jaccard\n",
    "jaccard_knn = jaccard_similarity_score(y_test, yhat_knn)\n",
    "print(\"KNN Jaccard index: \", jaccard_knn)\n",
    "\n",
    "# f1_score\n",
    "f1_score_knn = f1_score(y_test, yhat_knn, average='weighted')\n",
    "print(\"KNN F1-score: \", f1_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "severityTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "severityTree.fit(X_train, y_train)\n",
    "# predicted y\n",
    "yhat_dt = severityTree.predict(X_test)\n",
    "\n",
    "# jaccard\n",
    "jaccard_dt = jaccard_similarity_score(y_test, yhat_dt)\n",
    "print(\"DT Jaccard index: \", jaccard_dt)\n",
    "\n",
    "# f1_score\n",
    "f1_score_dt = f1_score(y_test, yhat_dt, average='weighted')\n",
    "print(\"DT F1-score: \", f1_score_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(C=0.01).fit(X_train,y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_lg = LR.predict(X_test)\n",
    "yhat_lg_prob = LR.predict_proba(X_test)\n",
    "\n",
    "# jaccard\n",
    "jaccard_lg = jaccard_similarity_score(y_test, yhat_lg)\n",
    "print(\"LR Jaccard index: \", jaccard_lg)\n",
    "\n",
    "# f1_score\n",
    "f1_score_lg = f1_score(y_test, yhat_lg, average='weighted')\n",
    "print(\"LR F1-score: \", f1_score_lg)\n",
    "\n",
    "# logloss\n",
    "logloss_lg = log_loss(y_test, yhat_lg_prob)\n",
    "print(\"LR log loss: \", logloss_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# training\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted y\n",
    "yhat_svm = clf.predict(X_test)\n",
    "\n",
    "# jaccard\n",
    "jaccard_svm = jaccard_similarity_score(y_test, yhat_svm)\n",
    "print(\"SVM Jaccard index: \", jaccard_svm)\n",
    "\n",
    "# f1_score\n",
    "f1_score_svm = f1_score(y_test, yhat_svm, average='weighted')\n",
    "print(\"SVM F1-score: \", f1_score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm          | Jaccard | F1-score | LogLoss |\n",
    "|--------------------|---------|----------|---------|\n",
    "| KNN                | 0.6215  | 0.5654   | NA      |\n",
    "| Decision Tree      | 0.6638  | 0.5297   | NA      |\n",
    "| SVM                | 0.6635  | 0.5296   | NA      |\n",
    "| LogisticRegression | 0.6638  | 0.5297   | 0.6377  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project and analysis are quite helpful for the Seattle transportation department. Before I did the analysis, I thought that maybe weather, road, and light condition may cause more accidents, the results showed that it was not correct. However, we do figure out that the accidents are highly related to some specific locations. Thus, the traffic management division could try to improve the safety instructions or some other factors that could reduce the accidents.\n",
    "\n",
    "Furthermore, there are some places which has more accidents during the dark time. For those places, adding lights might be a good solution to reduce the collisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this project was to predict the accident severity in Seattle which conditions have higher impact of those accidents in order to aid stakeholders or government in narrowing down the search for optimal solution for reduce collisions. We used 4 different algorithms to predict severity. We found out that Decision Tree and Logistic Regression have more accuracy score than others.\n",
    "\n",
    "Final decision on optimal solution will be made by stakeholders based on specific characteristics of conditions and locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
